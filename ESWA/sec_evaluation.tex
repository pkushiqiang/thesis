\section{Evaluation of the System}
\label{sec1}

\subsection{Comparing with other models}

In a traditional information retrieval systems, Precision@K and NDCG are widely used measures.
Precision@K is the proportion of relevant documents in the first K positions and is given below:
$$ P@k = \frac{1}{k} \sum^m_{i=1} l_i 1 \left(  r(i) \leq k  \right )  $$
Where 1 is the indicator function: $1(A) = 1$ if A is true, 0 otherwise.

To evaluate a job finding, we compare the results of the system with three classical information retrieval models: Kullback-Leibler divergence~\cite{zhai2008statistical},  TF-IDF~\cite{manning2008introduction} and Okapi BM25~\cite{robertson1995okapi}. We give the definition of these measures below.

Kullback-Leibler divergence is a non-symmetric measure of the difference between two probability distributions $P$ and $Q$. The score of a document $D$ with respect to query $Q$ is given by:
\begin{equation}
    \begin{array}{rcl}
        s(D,Q) & = & -D( \theta_Q \parallel  \theta_D )\\
               & = &- \sum_{ \omega \in V } p (\omega \mid \theta_Q) \log \frac{ p (\omega \mid \theta_Q )}{p(\omega \mid \theta_D)} \\

    \end{array}
\end{equation}

In the equation $\theta_Q$ is the language model for a query, and  $\theta_D$ is the language model for a document.

TF-IDF is a Vector Space Model, which calculates the similarity between the vectors of the query $q$ and the document $d$. In the equation, $tf$ is the term frequency, and $idf$ is the inverse document frequency. The TF-IDF weighting scheme assigned to term $t$ a weight in document $d$ given by:
$$ tf\text{-}idf_{t,d} = tf_{t,d} \times idf_{t} $$
The similarity between the query $q$ and the document $d$ given by:
$$ score(q,d) =  \sum_{t \in q }  tf\text{-}idf_{t,d} $$

Okapi BM25 is a bag-of-words retrieval model that ranks a set of documents based on the query terms appearing in each document. Given a query $q$, the BM25 score of a document $d$ is:

$$ score(q,d) = \sum_{ t \in q  }  idf_t \frac {tf_{t,d} \cdot(k_1 +1) }  {tf_{t,d} +k_1 \cdot ( 1-b + b\cdot \frac { \left | D \right |}{avgdl})}  $$

In the formula, $idf_t$ is IDF value of term $t$, $tf$ is the term frequency; $ |D|$ is the length of the document $D$; $avgdl$ is the average length of all the documents, and $k_1$ and $b$ are the free parameters.

In the evaluation phase, we created a data set of 100 job descriptions that includes several kinds of jobs such as web developers, server back-end developers, mobile developers and so on. We used 5 candidate r\'esum\'es and retrieved the top 20 jobs.  The relevance value of the job descriptions to each r\'esum\'e were set manually by ten human judge. We created a query $q$ from the r\'esum\'e, treated the text of the job descriptions as documents $d$, and applied standard ad-hoc retrieval techniques to rank the jobs. We intended to return jobs that better matched the candidates' r\'esum\'es at the top. The result of Precision@k is in table~\ref{tab:job_precision}.


\begin{table}[ht]
\caption{Precision of Job Ranking } % title of Table
\centering % used for centering table
\begin{tabular}{    | c | c | c | c | c |  }
 \hline
       k     & Okapi BM25 & KL    & TF-IDF   & R\'esuMatcher  \\
 \hline
       5     & 0.66       & 0.27  & 0.72     & 0.82   \\
 \hline
       10    & 0.46       & 0.27  & 0.50     & 0.76   \\
 \hline
       20    & 0.33       & 0.21  & 0.35     & 0.77   \\
 \hline

\end{tabular}
\label{tab:job_precision} % is used to refer this table in the text
\\Our system get the highest precision.
\end{table}

The other measure we used is NDCG, which is explained in last section. Table~\ref{tab:job_ndcg} shows the NDCG value get from different information retrieval models. The result shows that Ontology Similarity  performs the best.

\begin{table}[ht]
\caption{NDCG of Job Ranking } % title of Table
\centering % used for centering table
\begin{tabular}{    | c | c | c | c | c |  }
 \hline
       k    & Okapi BM25 & KL    & TF-IDF & R\'esuMatcher  \\
 \hline
       5    & 0.15       & 0.34  & 0.45     & 0.78   \\
 \hline
       10   & 0.18       & 0.44  & 0.47     & 0.72   \\
 \hline
       20   & 0.19       & 0.35  & 0.45     & 0.66   \\
 \hline

\end{tabular}
\label{tab:job_ndcg} % is used to refer this table in the text
\\Our sytem get the highest NDCG value.
\end{table}

\subsection{Comparing with the Keyword Searching}
We also made experiments to compare the quality of search results quality between our system and the keyword searching approach. In one experiment, we first selected a r\'esum\'e, and picked a related keyword e.g. ``Java''  to search jobs in Indeed.com. We used this process to simulate how a job seeker searches jobs on Indeed.com. The top 100 jobs returned by Indeed.com were saved in our system, then we used the r\'esum\'e to match the 100 jobs. We compared the quality of the approaches by calculating the measures Precision@K  and  DCG. We use DCG not NDCG because we compare the two approaches on the same data set, and the IDCG is the same. Five judges gave the relevance values to the r\'esum\'es and jobs, and we used average values. Table~\ref{tab:comparison_java} shows comparison between the two approaches by using keyword ``Java'' and the r\'esum\'e of a Java developer. The comparison for  keyword ``Python'' and the r\'esum\'e of a Python developer is shown in Table~\ref{tab:comparison_python}, and the average comparison for five keywords and five r\'esum\'e is shown in Table~\ref{tab:comparison_avg}. We can see from the result that our system can get much better results than keyword searching approach.

\begin{table}[ht]
\caption{Comparison of the Two Approaches - Java Developer  } % title of Table
\centering % used for centering table
\begin{tabular}{    | c | c | c | c | c |  }
 \hline
  \multirow{2}{*}k    & \multicolumn{2}{c|}{Precision@K }    & \multicolumn{2}{c|}{DCG } \\
 \cline{2-5}
            & Indeed  & Our System  & Indeed     & R\'esuMatcher   \\
 \hline
       5    & 0.7     & 1               & 10.86       & 27.785   \\
 \hline
       10   & 0.65    & 0.95            & 24.85       & 47.06   \\
 \hline
       20   & 0.7     & 0.825           & 51.97       & 73.33   \\
 \hline

\end{tabular}
\label{tab:comparison_java} % is used to refer this table in the text
\\Our system can get better result.
\end{table}



\begin{table}[ht]
\caption{Comparison of the Two Approaches - Python Developer  } % title of Table
\centering % used for centering table
\begin{tabular}{    | c | c | c | c | c |  }
 \hline
  \multirow{2}{*}k    & \multicolumn{2}{c|}{Precision@K }    & \multicolumn{2}{c|}{DCG } \\
 \cline{2-5}
            & Indeed  & Our System  & Indeed     & R\'esuMatcher   \\
 \hline
       5    & 0.4    & 0.6               & 11.27       & 11.98   \\
 \hline
       10   & 0.3    & 0.6               & 13.43       & 15.79   \\
 \hline
       20   & 0.2    & 0.4               & 16.44       & 20.21   \\
 \hline

\end{tabular}
\label{tab:comparison_python} % is used to refer this table in the text
\\Our system can get better result.
\end{table}

\begin{table}[ht]
\caption{Comparison of the Two Approaches - Average  } % title of Table
\centering % used for centering table
\begin{tabular}{    | c | c | c | c | c |  }
 \hline
  \multirow{2}{*}k    & \multicolumn{2}{c|}{Precision@K }    & \multicolumn{2}{c|}{DCG } \\
 \cline{2-5}
            & Indeed  & Our System  & Indeed     & R\'esuMatcher   \\
 \hline
       5    & 0.84    & 0.87               & 23.87       & 32.97   \\
 \hline
       10   & 0.72    & 0.86               & 37.02       & 45.57   \\
 \hline
       20   & 0.645   & 0.768              & 58.70       & 66.70   \\
 \hline

\end{tabular}
\label{tab:comparison_avg} % is used to refer this table in the text
\\Our system can get better result.
\end{table}
